{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa834df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.040682Z",
     "iopub.status.busy": "2026-01-20T01:29:07.040345Z",
     "iopub.status.idle": "2026-01-20T01:29:07.345496Z",
     "shell.execute_reply": "2026-01-20T01:29:07.345144Z"
    },
    "papermill": {
     "duration": 0.314077,
     "end_time": "2026-01-20T01:29:07.346504",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.032427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7250e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003004,
     "end_time": "2026-01-20T01:29:07.352669",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.349665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Torchvision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5836254f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.358881Z",
     "iopub.status.busy": "2026-01-20T01:29:07.358712Z",
     "iopub.status.idle": "2026-01-20T01:29:07.372235Z",
     "shell.execute_reply": "2026-01-20T01:29:07.371989Z"
    },
    "papermill": {
     "duration": 0.017542,
     "end_time": "2026-01-20T01:29:07.373006",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.355464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torchvision_dictlist = [\n",
    "    \n",
    "    {'model': 'alexnet', 'model_display_name': 'AlexNet', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'vgg11', 'model_display_name': 'VGG11', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13', 'model_display_name': 'VGG13', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16', 'model_display_name': 'VGG16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19', 'model_display_name': 'VGG19', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg11_bn', 'model_display_name': 'VGG11-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13_bn', 'model_display_name': 'VGG13-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16_bn', 'model_display_name': 'VGG16-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19_bn', 'model_display_name': 'VGG19-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet18', 'model_display_name': 'ResNet18', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet34', 'model_display_name': 'ResNet34', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet50', 'model_display_name': 'ResNet50', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet101', 'model_display_name': 'ResNet101', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet152', 'model_display_name': 'ResNet152', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_0', 'model_display_name': 'SqueezeNet1.0', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_1', 'model_display_name': 'SqueezeNet1.1', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet121', 'model_display_name': 'DenseNet121', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet161', 'model_display_name': 'DenseNet161', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet169', 'model_display_name': 'DenseNet169', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet201', 'model_display_name': 'DenseNet201', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'googlenet', 'model_display_name': 'GoogleNet', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'shufflenet_v2_x0_5', 'model_display_name': 'ShuffleNet-V2-x0.5', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    " \n",
    "    {'model': 'shufflenet_v2_x1_0', 'model_display_name': 'ShuffleNet-V2-x1.0', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mobilenet_v2', 'model_display_name': 'MobileNet-V2', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext50_32x4d', 'model_display_name': 'ResNext50-32x4D', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext101_32x8d', 'model_display_name': 'ResNext50-32x8D', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet50_2', 'model_display_name': 'Wide-ResNet50', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet101_2', 'model_display_name': 'Wide-ResNet101', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet0_5', 'model_display_name': 'MNASNet0.5', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet1_0', 'model_display_name': 'MNASNet1.0', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "]\n",
    "\n",
    "\n",
    "train_type_text = {\n",
    "    'classification': 'image classification',\n",
    "    'video': 'video classification',\n",
    "    'segmentation': 'image segmentation',\n",
    "    'detection': 'keypoint detection'\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'kinetics400': 'Kinetics400',\n",
    "    'coco2017': 'CoCo2017',\n",
    "}\n",
    "\n",
    "torchvision_dictlist_ = copy(torchvision_dictlist)\n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    torchvision_dictlist_.append(dict_i_random)\n",
    "torchvision_dictlist = torchvision_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i = torchvision_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "torchvision_df = pd.DataFrame(torchvision_dictlist)\n",
    "torchvision_df['architecture'] = torchvision_df['model']\n",
    "torchvision_df['task_cluster'] = 'Semantic'\n",
    "torchvision_df['model_class'] = 'Convolutional'\n",
    "torchvision_df['model_source'] = 'torchvision'\n",
    "torchvision_df['model_source_url'] = 'pytorch.org/docs/stable/torchvision/models.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371c1d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.379886Z",
     "iopub.status.busy": "2026-01-20T01:29:07.379785Z",
     "iopub.status.idle": "2026-01-20T01:29:07.390535Z",
     "shell.execute_reply": "2026-01-20T01:29:07.390317Z"
    },
    "papermill": {
     "duration": 0.015128,
     "end_time": "2026-01-20T01:29:07.391171",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.376043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mnasnet1_0</td>\n",
       "      <td>MNASNet1.0</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>MNASNet1.0 trained on image classification wit...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>mnasnet1_0</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>resnet34</td>\n",
       "      <td>ResNet34</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>ResNet34 trained on image classification with ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>resnet34</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>vgg11</td>\n",
       "      <td>VGG11</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>VGG11 randomly initialized, with no training.</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>vgg11</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model model_display_name      train_type train_data  \\\n",
       "29  mnasnet1_0         MNASNet1.0  classification   imagenet   \n",
       "10    resnet34           ResNet34  classification   imagenet   \n",
       "31       vgg11              VGG11          random       None   \n",
       "\n",
       "                                          description task_cluster  \\\n",
       "29  MNASNet1.0 trained on image classification wit...     Semantic   \n",
       "10  ResNet34 trained on image classification with ...     Semantic   \n",
       "31      VGG11 randomly initialized, with no training.     Semantic   \n",
       "\n",
       "   architecture    model_class model_source  \\\n",
       "29   mnasnet1_0  Convolutional  torchvision   \n",
       "10     resnet34  Convolutional  torchvision   \n",
       "31        vgg11  Convolutional  torchvision   \n",
       "\n",
       "                                   model_source_url  \n",
       "29  pytorch.org/docs/stable/torchvision/models.html  \n",
       "10  pytorch.org/docs/stable/torchvision/models.html  \n",
       "31  pytorch.org/docs/stable/torchvision/models.html  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9eeaab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.002828,
     "end_time": "2026-01-20T01:29:07.397376",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.394548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Timm Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88304d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.403876Z",
     "iopub.status.busy": "2026-01-20T01:29:07.403730Z",
     "iopub.status.idle": "2026-01-20T01:29:07.428889Z",
     "shell.execute_reply": "2026-01-20T01:29:07.428580Z"
    },
    "papermill": {
     "duration": 0.029627,
     "end_time": "2026-01-20T01:29:07.429786",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.400159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timm_dictlist = [\n",
    "    \n",
    "    {'model': 'botnet26t_256', 'model_display_name': 'BotNet-26T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_s24_224', 'model_display_name': 'CaiT-S24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_xxs24_224', 'model_display_name': 'CaiT-XXS24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'coat_lite_tiny', 'model_display_name': 'CoaT-Lite-Tiny',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'coat_lite_small', 'model_display_name': 'CoaT-Lite-Small',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'coat_mini', 'model_display_name': 'CoaT-Mini', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'convit_base', 'model_display_name': 'ConViT-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convit_small', 'model_display_name': 'ConViT-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convit_tiny', 'model_display_name': 'ConViT-T', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convmixer_768_32', 'model_display_name': 'ConvMixer-768-32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convnext_base', 'model_display_name': 'ConvNext-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_large', 'model_display_name': 'ConvNext-L', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_small', 'model_display_name': 'ConvNext-S', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'crossvit_base_240', 'model_display_name': 'CrossViT-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'crossvit_tiny_240', 'model_display_name': 'CrossViT-T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_base_patch16_224', 'model_display_name': 'DeiT-B-P16-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_small_patch16_224', 'model_display_name': 'DeiT-S-P16-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_tiny_patch16_224', 'model_display_name': 'DeiT-T-P16-224', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cspdarknet53', 'model_display_name': 'CSP-DarkNet53', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'cspresnet50', 'model_display_name': 'CSP-ResNet50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'dla34', 'model_display_name': 'DLA34',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'dla169', 'model_display_name': 'DLA169',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l0', 'model_display_name': 'ECA-NFNeT-L0',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l1', 'model_display_name': 'ECA-NFNeT-L1', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ecaresnet50d', 'model_display_name': 'ECA-Resnet50-D', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ecaresnet101d', 'model_display_name': 'ECA-Resnet101-D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b1', 'model_display_name': 'EfficientNet-B1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b3', 'model_display_name': 'EfficientNet-B3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b5', 'model_display_name': 'EfficientNet-B5',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b7', 'model_display_name': 'EfficientNet-B7',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gcresnet50t', 'model_display_name': 'GCResNet50T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnetv2_rw_s', 'model_display_name': 'EfficientNet-V2-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'fbnetc_100', 'model_display_name': 'FBNetC100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gernet_l', 'model_display_name': 'GerNet-L', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gernet_s', 'model_display_name': 'GerNet-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ghostnet_100', 'model_display_name': 'GhostNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gmixer_24_224', 'model_display_name': 'GMixer-24', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gmlp_s16_224', 'model_display_name': 'GMLP-S16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'halonet26t', 'model_display_name': 'HaloNet-26T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hardcorenas_a', 'model_display_name': 'HardCoreNAS-A',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hardcorenas_f', 'model_display_name': 'HardCoreNAS-F',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hrnet_w18', 'model_display_name': 'HRNet-W18',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hrnet_w64', 'model_display_name': 'HRNet-W64',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'jx_nest_base', 'model_display_name': 'JX-NesT-Base',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'jx_nest_small', 'model_display_name': 'JX-NesT-Small',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'jx_nest_tiny', 'model_display_name': 'JX-NesT-Tiny',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'lambda_resnet26t', 'model_display_name': 'Lambda-ResNet-26T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'levit_128', 'model_display_name': 'LeViT128',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'levit_256', 'model_display_name': 'LeViT256',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'inception_resnet_v2', 'model_display_name': 'Inception-Resnet-V2',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'inception_v3', 'model_display_name': 'Inception-V3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'inception_v4', 'model_display_name': 'Inception-V4',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mixer_b16_224', 'model_display_name': 'MLP-Mixer-B16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "\n",
    "    {'model': 'mixer_l16_224', 'model_display_name': 'MLP-Mixer-L16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'mixnet_l', 'model_display_name': 'MixNet-L',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mixnet_s', 'model_display_name': 'MixNet-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "        \n",
    "    {'model': 'mobilenetv3_large_100', 'model_display_name': 'MobileNet-V3-Large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_rw', 'model_display_name': 'MobileNet-V3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mobilevit_s', 'model_display_name': 'MobileViT-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'nasnetalarge', 'model_display_name': 'NASNet-A-Large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'nf_resnet50', 'model_display_name': 'NF-ResNet50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'nfnet_l0', 'model_display_name': 'NF-Net-L0',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'pit_b_224', 'model_display_name': 'PiT-B-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pit_s_224', 'model_display_name': 'PiT-S-224', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pit_ti_224', 'model_display_name': 'PiT-T-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pnasnet5large', 'model_display_name': 'PNASNet-5-Large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'poolformer_s12', 'model_display_name': 'PoolFormer-S12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'poolformer_s36', 'model_display_name': 'PoolFormer-S36',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'poolformer_m36', 'model_display_name': 'PoolFormer-M36',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pit_ti_224', 'model_display_name': 'PiT-T-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'regnetx_064', 'model_display_name': 'RegNetX-64',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'regnety_064', 'model_display_name': 'RegNetY-64',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'repvgg_b1', 'model_display_name': 'RepVGG-B1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'repvgg_b1g4', 'model_display_name': 'RepVGG-B1G4',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'res2net50_26w_4s', 'model_display_name': 'Res2Net50-26W-4S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resmlp_12_224', 'model_display_name': 'ResMLP-12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resmlp_24_224', 'model_display_name': 'ResMLP-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resmlp_36_224', 'model_display_name': 'ResMLP-36',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resmlp_big_24_224', 'model_display_name': 'ResMLP-Big-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resnest50d', 'model_display_name': 'ResNest50D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetrs50', 'model_display_name': 'ResNetRS50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'rexnet_100', 'model_display_name': 'RexNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'selecsls60', 'model_display_name': 'SelecSLS60',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'semnasnet_100', 'model_display_name': 'SemNASNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'seresnet50', 'model_display_name': 'SEResNet50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'seresnext50_32x4d', 'model_display_name': 'SEResNext50-32x4D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'skresnet18', 'model_display_name': 'SKResNet18',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'skresnext50_32x4d', 'model_display_name': 'SKResNext50-32x4D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'spnasnet_100', 'model_display_name': 'SPNasNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window7_224', 'model_display_name': 'Swin-B-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window7_224', 'model_display_name': 'Swin-L-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_small_patch4_window7_224', 'model_display_name': 'Swin-S-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_tiny_patch4_window7_224', 'model_display_name': 'Swin-T-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b0', 'model_display_name': 'TF-EfficientNet-B0',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b1', 'model_display_name': 'TF-EfficientNet-B1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b2', 'model_display_name': 'TF-EfficientNet-B2',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b3', 'model_display_name': 'TF-EfficientNet-B3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b4', 'model_display_name': 'TF-EfficientNet-B4',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b5', 'model_display_name': 'TF-EfficientNet-B5',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b6', 'model_display_name': 'TF-EfficientNet-B6',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b7', 'model_display_name': 'TF-EfficientNet-B7',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tnt_s_patch16_224', 'model_display_name': 'TnT-P16-224', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'tresnet_l', 'model_display_name': 'TResNet-L',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'twins_pcpvt_base', 'model_display_name': 'Twins-PCPVT-B', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'twins_svt_base', 'model_display_name': 'Twins-SVT-B', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'visformer_small', 'model_display_name': 'Visformer',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_224', 'model_display_name': 'ViT-L-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_224', 'model_display_name': 'ViT-S-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_tiny_patch16_224', 'model_display_name': 'ViT-T-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224', 'model_display_name': 'ViT-B-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_224', 'model_display_name': 'ViT-B-P32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch32_224', 'model_display_name': 'ViT-L-P32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_224', 'model_display_name': 'ViT-S-P32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d1_224', 'model_display_name': 'VoLo-D1', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d3_224', 'model_display_name': 'VoLo-D3', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d5_224', 'model_display_name': 'VoLo-D5', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xception', 'model_display_name': 'XCeption',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'xception65', 'model_display_name': 'XCeption65',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p8_224', 'model_display_name': 'XCIT-N-12-P8', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_small_12_p8_224', 'model_display_name': 'XCIT-S-12-P8', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_medium_24_p8_224', 'model_display_name': 'XCIT-M-24-P8', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p16_224', 'model_display_name': 'XCIT-N-12-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_small_12_p16_224', 'model_display_name': 'XCIT-S-12-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_medium_24_p16_224', 'model_display_name': 'XCIT-M-24-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'classification': 'image classification',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}\n",
    "\n",
    "timm_dictlist_ = copy(timm_dictlist)\n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    timm_dictlist_.append(dict_i_random)\n",
    "timm_dictlist = timm_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i = timm_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "timm_df = pd.DataFrame(timm_dictlist)\n",
    "timm_df['architecture'] = timm_df['model']\n",
    "timm_df['task_cluster'] = 'Semantic'\n",
    "timm_df['model_source'] = 'timm'\n",
    "timm_df['model_source_url'] = 'https://github.com/rwightman/pytorch-image-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fcbf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.436499Z",
     "iopub.status.busy": "2026-01-20T01:29:07.436368Z",
     "iopub.status.idle": "2026-01-20T01:29:07.446995Z",
     "shell.execute_reply": "2026-01-20T01:29:07.446747Z"
    },
    "papermill": {
     "duration": 0.015007,
     "end_time": "2026-01-20T01:29:07.447737",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.432730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>inception_v3</td>\n",
       "      <td>Inception-V3</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Inception-V3 trained on image classification w...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>inception_v3</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>tf_efficientnet_b3</td>\n",
       "      <td>TF-EfficientNet-B3</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>TF-EfficientNet-B3 randomly initialized, with ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>tf_efficientnet_b3</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>nf_resnet50</td>\n",
       "      <td>NF-ResNet50</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>NF-ResNet50 trained on image classification wi...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>nf_resnet50</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  model_display_name      train_type train_data  \\\n",
       "50         inception_v3        Inception-V3  classification   imagenet   \n",
       "224  tf_efficientnet_b3  TF-EfficientNet-B3          random       None   \n",
       "62          nf_resnet50         NF-ResNet50  classification   imagenet   \n",
       "\n",
       "       model_class                                        description  \\\n",
       "50   Convolutional  Inception-V3 trained on image classification w...   \n",
       "224  Convolutional  TF-EfficientNet-B3 randomly initialized, with ...   \n",
       "62   Convolutional  NF-ResNet50 trained on image classification wi...   \n",
       "\n",
       "    task_cluster        architecture model_source  \\\n",
       "50      Semantic        inception_v3         timm   \n",
       "224     Semantic  tf_efficientnet_b3         timm   \n",
       "62      Semantic         nf_resnet50         timm   \n",
       "\n",
       "                                      model_source_url  \n",
       "50   https://github.com/rwightman/pytorch-image-mod...  \n",
       "224  https://github.com/rwightman/pytorch-image-mod...  \n",
       "62   https://github.com/rwightman/pytorch-image-mod...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e5202",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.002985,
     "end_time": "2026-01-20T01:29:07.454009",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.451024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Taskonomy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9bef6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.460617Z",
     "iopub.status.busy": "2026-01-20T01:29:07.460438Z",
     "iopub.status.idle": "2026-01-20T01:29:07.473517Z",
     "shell.execute_reply": "2026-01-20T01:29:07.473279Z"
    },
    "papermill": {
     "duration": 0.01745,
     "end_time": "2026-01-20T01:29:07.474339",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.456889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "taskonomy_dictlist = [\n",
    "    \n",
    "    {'model': 'autoencoding', 'model_display_name': 'Autoencoder', 'task_cluster': '2D',\n",
    "        'description': 'Image compression and decompression'},\n",
    "    \n",
    "    {'model': 'class_object', 'model_display_name': 'Object Classification', 'task_cluster': 'Semantic',\n",
    "        'description': '1000-way object classification (via knowledge distillation from ImageNet).'},\n",
    "    \n",
    "    {'model': 'class_scene', 'model_display_name': 'Scene Classification', 'task_cluster': 'Semantic',\n",
    "        'description': 'Scene Classification (via knowledge distillation from MIT Places).'},\n",
    "    \n",
    "    {'model': 'curvature', 'model_display_name': 'Curvatures', 'task_cluster': '3D',\n",
    "        'description': 'Magnitude of 3D principal curvatures'},\n",
    "    \n",
    "    {'model': 'colorization', 'model_display_name': 'Colorization', 'task_cluster': None,\n",
    "        'description': 'Colorizing input grayscale images.'},\n",
    "    \n",
    "    {'model': 'denoising', 'model_display_name': 'Denoising', 'task_cluster': 'Other',\n",
    "        'description': 'Uncorrupted version of corrupted image.'},\n",
    "    \n",
    "    {'model': 'depth_euclidean', 'model_display_name': 'Euclidean Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation'},\n",
    "    \n",
    "    {'model': 'depth_zbuffer', 'model_display_name': 'Z-Buffer Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation.'},\n",
    "    \n",
    "    {'model': 'edge_occlusion', 'model_display_name': 'Occlusion Edges', 'task_cluster': '3D',\n",
    "        'description': 'Edges which include parts of the scene.'},\n",
    "    \n",
    "    {'model': 'edge_texture', 'model_display_name': 'Texture Edges', 'task_cluster': '2D',\n",
    "        'description': 'Edges computed from RGB only (texture edges).'},\n",
    "    \n",
    "    {'model': 'egomotion', 'model_display_name': 'Egomotion', 'task_cluster': 'Geometric',\n",
    "        'description': 'Odometry (camera poses) given three input images.'},\n",
    "    \n",
    "    {'model': 'fixated_pose', 'model_display_name': 'Camera Pose (Fixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with matching optical centers.'},\n",
    "    \n",
    "    {'model': 'inpainting', 'model_display_name': 'Inpainting', 'task_cluster': '2D',\n",
    "        'description': 'Filling in masked center of image.'},\n",
    "    \n",
    "    {'model': 'jigsaw', 'model_display_name': 'Jigsaw', 'task_cluster': 'Geometric',\n",
    "        'description': 'Putting scrambled image pieces back together.'},\n",
    "    \n",
    "    {'model': 'keypoints2d', 'model_display_name': '2D Keypoints', 'task_cluster': '2D',\n",
    "        'description': 'Keypoint estimation from RGB-only (texture features).'},\n",
    "    \n",
    "    {'model': 'keypoints3d', 'model_display_name': '3D Keypoints', 'task_cluster': '3D',\n",
    "        'description': '3D Keypoint estimation from underlying scene 3D.'},\n",
    "    \n",
    "    {'model': 'nonfixated_pose', 'model_display_name': 'Camera Pose (Nonfixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with distinct optical centers.'},\n",
    "    \n",
    "    {'model': 'normal', 'model_display_name': 'Surface Normals', 'task_cluster': '3D',\n",
    "        'description': 'Pixel-wise surface normals.'},\n",
    "    \n",
    "    {'model': 'point_matching', 'model_display_name': 'Point Matching', 'task_cluster': 'Geometric',\n",
    "        'description': 'Classifying if centers of two images match or not.'},\n",
    "    \n",
    "    {'model': 'reshading', 'model_display_name': 'Reshading', 'task_cluster': '3D',\n",
    "        'description': 'Reshading with new lighting placed at camera location.'},\n",
    "    \n",
    "    {'model': 'room_layout', 'model_display_name': 'Room Layout', 'task_cluster': 'Geometric',\n",
    "        'description': 'Orientation and aspect ratio of cubic room layout.'},\n",
    "    \n",
    "    {'model': 'segment_semantic', 'model_display_name': 'Semantic Segmentation', 'task_cluster': 'Semantic',\n",
    "        'description': 'Pixel-wise semantic labeling (via knowledge distillation from MS COCO).'},\n",
    "    \n",
    "    {'model': 'segment_unsup25d', 'model_display_name': 'Unsupervised 2.5D Segmentation', 'task_cluster': '3D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB-D-Normals-Curvature image.'},\n",
    "    \n",
    "    {'model': 'segment_unsup2d', 'model_display_name': 'Unsupervised 2D Segmentation', 'task_cluster': '2D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB.'},\n",
    "    \n",
    "    {'model': 'vanishing_point', 'model_display_name': 'Vanishing Point', 'task_cluster': 'Geometric',\n",
    "        'description': 'Three Manhattan-world vanishing points.'}\n",
    "]\n",
    "\n",
    "taskonomy_df = pd.DataFrame(taskonomy_dictlist)\n",
    "problematic_models = ['colorization']\n",
    "taskonomy_df = taskonomy_df[~taskonomy_df.model.isin(problematic_models)]\n",
    "taskonomy_df['architecture'] = 'resnet50'\n",
    "taskonomy_df['train_type'] = 'taskonomy'\n",
    "taskonomy_df['train_data'] = 'taskonomy'\n",
    "\n",
    "task_random = {'model': 'random_weights', 'model_display_name': 'Random Weights', \n",
    "               'architecture': 'resnet50', 'train_type': 'taskonomy', 'train_data': None, \n",
    "               'task_cluster': 'Random', 'description': 'Taskonomy architecture randomly initialized.'}\n",
    "\n",
    "task_random_df = pd.DataFrame(task_random,  index = [taskonomy_df.shape[0] + 1])\n",
    "\n",
    "taskonomy_df = pd.concat([taskonomy_df, task_random_df])\n",
    "\n",
    "taskonomy_df['model_class'] = 'Convolutional'\n",
    "taskonomy_df['model_source'] = 'taskonomy'\n",
    "taskonomy_df['model_source_url'] = 'github.com/alexsax/midlevel-reps/tree/visualpriors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43db6ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.481980Z",
     "iopub.status.busy": "2026-01-20T01:29:07.481881Z",
     "iopub.status.idle": "2026-01-20T01:29:07.491279Z",
     "shell.execute_reply": "2026-01-20T01:29:07.491032Z"
    },
    "papermill": {
     "duration": 0.014142,
     "end_time": "2026-01-20T01:29:07.492006",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.477864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>edge_occlusion</td>\n",
       "      <td>Occlusion Edges</td>\n",
       "      <td>3D</td>\n",
       "      <td>Edges which include parts of the scene.</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>keypoints3d</td>\n",
       "      <td>3D Keypoints</td>\n",
       "      <td>3D</td>\n",
       "      <td>3D Keypoint estimation from underlying scene 3D.</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>normal</td>\n",
       "      <td>Surface Normals</td>\n",
       "      <td>3D</td>\n",
       "      <td>Pixel-wise surface normals.</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model model_display_name task_cluster  \\\n",
       "8   edge_occlusion    Occlusion Edges           3D   \n",
       "15     keypoints3d       3D Keypoints           3D   \n",
       "17          normal    Surface Normals           3D   \n",
       "\n",
       "                                         description architecture train_type  \\\n",
       "8            Edges which include parts of the scene.     resnet50  taskonomy   \n",
       "15  3D Keypoint estimation from underlying scene 3D.     resnet50  taskonomy   \n",
       "17                       Pixel-wise surface normals.     resnet50  taskonomy   \n",
       "\n",
       "   train_data    model_class model_source  \\\n",
       "8   taskonomy  Convolutional    taskonomy   \n",
       "15  taskonomy  Convolutional    taskonomy   \n",
       "17  taskonomy  Convolutional    taskonomy   \n",
       "\n",
       "                                     model_source_url  \n",
       "8   github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "15  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "17  github.com/alexsax/midlevel-reps/tree/visualpr...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskonomy_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c378bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.002913,
     "end_time": "2026-01-20T01:29:07.498323",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.495410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b7d53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.505176Z",
     "iopub.status.busy": "2026-01-20T01:29:07.504964Z",
     "iopub.status.idle": "2026-01-20T01:29:07.513875Z",
     "shell.execute_reply": "2026-01-20T01:29:07.513620Z"
    },
    "papermill": {
     "duration": 0.01344,
     "end_time": "2026-01-20T01:29:07.514644",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.501204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_dictlist = [\n",
    "    \n",
    "    {'model': 'RN50', 'model_display_name': 'CLiP-ResNet50', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN101', 'model_display_name': 'CLiP-ResNet101', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x4', 'model_display_name': 'CLiP-ResNet50x4', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x16', 'model_display_name': 'CLiP-ResNet50x16', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x64', 'model_display_name': 'CLiP-ResNet50x64', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ViT-B/32', 'model_display_name': 'CLiP-ViT-B/32', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ViT-B/16', 'model_display_name': 'CLiP-ViT-B/32', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ViT-L/14', 'model_display_name': 'CLiP-ViT-L/14', 'model_class': 'Transformer'},\n",
    "    \n",
    "]\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(clip_dictlist):\n",
    "    dict_i = clip_dictlist[i]\n",
    "    dict_i['architecture'] = dict_i['model'].replace('RN','ResNet')\n",
    "    dict_i['description'] = '{}, a hybrid vision-language model.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "clip_df = pd.DataFrame(clip_dictlist)\n",
    "clip_df['train_type'] = 'clip'\n",
    "clip_df['train_data'] = 'openai400M'\n",
    "clip_df['task_cluster'] = 'Vision-Language'\n",
    "clip_df['model_source'] = 'clip'\n",
    "clip_df['model_source_url'] = 'https://github.com/openai/CLIP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f70e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.521962Z",
     "iopub.status.busy": "2026-01-20T01:29:07.521740Z",
     "iopub.status.idle": "2026-01-20T01:29:07.531356Z",
     "shell.execute_reply": "2026-01-20T01:29:07.531101Z"
    },
    "papermill": {
     "duration": 0.013931,
     "end_time": "2026-01-20T01:29:07.532044",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.518113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>architecture</th>\n",
       "      <th>description</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RN50x4</td>\n",
       "      <td>CLiP-ResNet50x4</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50x4</td>\n",
       "      <td>CLiP-ResNet50x4, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>CLiP-ViT-L/14</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>CLiP-ViT-L/14, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RN50</td>\n",
       "      <td>CLiP-ResNet50</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>CLiP-ResNet50, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model model_display_name    model_class architecture  \\\n",
       "2    RN50x4    CLiP-ResNet50x4  Convolutional   ResNet50x4   \n",
       "7  ViT-L/14      CLiP-ViT-L/14    Transformer     ViT-L/14   \n",
       "0      RN50      CLiP-ResNet50  Convolutional     ResNet50   \n",
       "\n",
       "                                        description train_type  train_data  \\\n",
       "2  CLiP-ResNet50x4, a hybrid vision-language model.       clip  openai400M   \n",
       "7    CLiP-ViT-L/14, a hybrid vision-language model.       clip  openai400M   \n",
       "0    CLiP-ResNet50, a hybrid vision-language model.       clip  openai400M   \n",
       "\n",
       "      task_cluster model_source                model_source_url  \n",
       "2  Vision-Language         clip  https://github.com/openai/CLIP  \n",
       "7  Vision-Language         clip  https://github.com/openai/CLIP  \n",
       "0  Vision-Language         clip  https://github.com/openai/CLIP  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25918aeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003744,
     "end_time": "2026-01-20T01:29:07.539352",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.535608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### VISSL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c9b428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.546657Z",
     "iopub.status.busy": "2026-01-20T01:29:07.546548Z",
     "iopub.status.idle": "2026-01-20T01:29:07.557329Z",
     "shell.execute_reply": "2026-01-20T01:29:07.557021Z"
    },
    "papermill": {
     "duration": 0.015428,
     "end_time": "2026-01-20T01:29:07.558141",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.542713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vissl_dictlist = [\n",
    "    {'model': 'ResNet50-JigSaw-P100',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/jigsaw_rn50_in1k_ep105_perm2k_jigsaw_8gpu_resnet_17_07_20.db174a43/model_final_checkpoint_phase104.torch'},\n",
    " \n",
    "  #   {'model': 'ResNet50-Colorization-Goyal19',\n",
    "  # 'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_colorization_in1k_goyal19.torch'},\n",
    "    \n",
    "    {'model': 'ResNet50-JigSaw-Goyal19',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_jigsaw_in1k_goyal19.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-RotNet',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/rotnet_rn50_in1k_ep105_rotnet_8gpu_resnet_17_07_20.46bada9f/model_final_checkpoint_phase125.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-ClusterFit-16K-RotNet',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_rotnet_16kclusters_in1k_ep105.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-NPID-32KNegative',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/npid_pp/4node_800ep_32kneg_cosine_resnet_23_07_20.75432662/model_final_checkpoint_phase799.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-PIRL',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/pirl_jigsaw_4node_pirl_jigsaw_4node_resnet_22_07_20.34377f59/model_final_checkpoint_phase799.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-SimCLR',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/simclr_rn50_1000ep_simclr_8node_resnet_16_07_20.afe428c7/model_final_checkpoint_phase999.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-DeepClusterV2-2x224',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_400ep_2x224_pretrain.pth.tar'},\n",
    " \n",
    "    {'model': 'ResNet50-DeepClusterV2-2x224+6x96',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_800ep_pretrain.pth.tar'},\n",
    " \n",
    "    {'model': 'ResNet50-SwAV-BS4096-2x224',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_in1k_rn50_400ep_swav_8node_resnet_27_07_20.a5990fc9/model_final_checkpoint_phase399.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-SwAV-BS4096-2x224+6x96',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_8node_2x224_rn50_in1k_swav_8node_resnet_30_07_20.c8fd7169/model_final_checkpoint_phase399.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-MoCoV2-BS256',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/moco_v2_1node_lr.03_step_b32_zero_init/model_final_checkpoint_phase199.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-BarlowTwins-BS2048',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/barlow_twins/barlow_twins_32gpus_4node_imagenet1k_1000ep_resnet50.torch'}\n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(vissl_dictlist):\n",
    "    dict_i = vissl_dictlist[i]\n",
    "    dict_i['model_display_name'] = dict_i['model']\n",
    "    dict_i['description'] = '{}, a self-supervised representation learner trained on ImageNet.'.format(dict_i['model'])\n",
    "    \n",
    "vissl_df = pd.DataFrame(vissl_dictlist)\n",
    "vissl_df['architecture'] = 'ResNet50'\n",
    "vissl_df['train_type'] = 'selfsupervised'\n",
    "vissl_df['train_data'] = 'imagenet'\n",
    "vissl_df['task_cluster'] = 'Self-Supervised'\n",
    "vissl_df['model_source'] = 'vissl'\n",
    "vissl_df['model_class'] = 'Convolutional'\n",
    "vissl_df['model_source_url'] = 'https://github.com/facebookresearch/vissl/'\n",
    "\n",
    "vissl_df['model_display_name'] = vissl_df['model_display_name'].apply(lambda x: x.replace('-2x224', ''))\n",
    "vissl_df['model_display_name'] = vissl_df['model_display_name'].apply(lambda x: x.replace('+6x96', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0b8537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.564994Z",
     "iopub.status.busy": "2026-01-20T01:29:07.564894Z",
     "iopub.status.idle": "2026-01-20T01:29:07.574697Z",
     "shell.execute_reply": "2026-01-20T01:29:07.574450Z"
    },
    "papermill": {
     "duration": 0.013726,
     "end_time": "2026-01-20T01:29:07.575332",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.561606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>weights_url</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ResNet50-SimCLR</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zoo...</td>\n",
       "      <td>ResNet50-SimCLR</td>\n",
       "      <td>ResNet50-SimCLR, a self-supervised representat...</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zoo...</td>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>ResNet50-PIRL, a self-supervised representatio...</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ResNet50-BarlowTwins-BS2048</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zo...</td>\n",
       "      <td>ResNet50-BarlowTwins-BS2048</td>\n",
       "      <td>ResNet50-BarlowTwins-BS2048, a self-supervised...</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  \\\n",
       "6               ResNet50-SimCLR   \n",
       "5                 ResNet50-PIRL   \n",
       "12  ResNet50-BarlowTwins-BS2048   \n",
       "\n",
       "                                          weights_url  \\\n",
       "6   https://dl.fbaipublicfiles.com/vissl/model_zoo...   \n",
       "5   https://dl.fbaipublicfiles.com/vissl/model_zoo...   \n",
       "12   https://dl.fbaipublicfiles.com/vissl/model_zo...   \n",
       "\n",
       "             model_display_name  \\\n",
       "6               ResNet50-SimCLR   \n",
       "5                 ResNet50-PIRL   \n",
       "12  ResNet50-BarlowTwins-BS2048   \n",
       "\n",
       "                                          description architecture  \\\n",
       "6   ResNet50-SimCLR, a self-supervised representat...     ResNet50   \n",
       "5   ResNet50-PIRL, a self-supervised representatio...     ResNet50   \n",
       "12  ResNet50-BarlowTwins-BS2048, a self-supervised...     ResNet50   \n",
       "\n",
       "        train_type train_data     task_cluster model_source    model_class  \\\n",
       "6   selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "5   selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "12  selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "\n",
       "                              model_source_url  \n",
       "6   https://github.com/facebookresearch/vissl/  \n",
       "5   https://github.com/facebookresearch/vissl/  \n",
       "12  https://github.com/facebookresearch/vissl/  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vissl_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb14da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003263,
     "end_time": "2026-01-20T01:29:07.582240",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.578977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dino Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e4d9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.589251Z",
     "iopub.status.busy": "2026-01-20T01:29:07.589131Z",
     "iopub.status.idle": "2026-01-20T01:29:07.598445Z",
     "shell.execute_reply": "2026-01-20T01:29:07.598153Z"
    },
    "papermill": {
     "duration": 0.01405,
     "end_time": "2026-01-20T01:29:07.599169",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.585119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dino_models = ['dino_vits16',\n",
    "               'dino_vits8',\n",
    "               'dino_vitb16',\n",
    "               'dino_vitb8',\n",
    "               'dino_xcit_small_12_p16',\n",
    "               'dino_xcit_small_12_p8',\n",
    "               'dino_xcit_medium_24_p16',\n",
    "               'dino_xcit_medium_24_p8',\n",
    "               'dino_resnet50']\n",
    "\n",
    "\n",
    "dino_dictlist = [\n",
    "    \n",
    "    {'model': 'dino_vits16', 'model_display_name': 'Dino-VIT-S16'},\n",
    "    \n",
    "    {'model': 'dino_vits8', 'model_display_name': 'Dino-VIT-S8'},\n",
    "    \n",
    "    {'model': 'dino_vitb16', 'model_display_name': 'Dino-VIT-B16'},\n",
    "    \n",
    "    {'model': 'dino_vitb8', 'model_display_name': 'Dino-VIT-B8'},\n",
    "    \n",
    "    {'model': 'dino_xcit_small_12_p16', 'model_display_name': 'Dino-XCIT-S12-P16'},\n",
    "    \n",
    "    {'model': 'dino_xcit_small_12_p8', 'model_display_name': 'Dino-XCIT-S12-P8'},\n",
    "    \n",
    "    {'model': 'dino_xcit_medium_24_p16', 'model_display_name': 'Dino-XCIT-M24-P16'},\n",
    "    \n",
    "    {'model': 'dino_xcit_medium_24_p8', 'model_display_name': 'Dino-XCIT-M24-P8'},\n",
    "    \n",
    "    {'model': 'dino_resnet50', 'model_display_name': 'Dino-ResNet50'}\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'selfsupervised': 'self supervision',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}   \n",
    "    \n",
    "for i, dict_i in enumerate(dino_dictlist):\n",
    "    dict_i = dino_dictlist[i]\n",
    "    dict_i['train_type'] = 'selfsupervised'\n",
    "    model_class = 'Transformer'\n",
    "    if 'resnet50' in dict_i['model']:\n",
    "        model_class = 'Convolutional'\n",
    "    dict_i['architecture'] = '_'.join(dict_i['model'].split('_')[1:])\n",
    "    dict_i['model_class'] = model_class\n",
    "    dict_i['train_data'] = 'imagenet'\n",
    "    dict_i['description'] = '{} trained via {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']],\n",
    "                                                                           train_data_text[dict_i['train_data']])\n",
    "    \n",
    "dino_df = pd.DataFrame(dino_dictlist)\n",
    "dino_df['task_cluster'] = 'SelfSupervised'\n",
    "dino_df['model_source'] = 'dino'\n",
    "dino_df['model_source_url'] = 'https://github.com/facebookresearch/dino'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8df5dfaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.606289Z",
     "iopub.status.busy": "2026-01-20T01:29:07.606192Z",
     "iopub.status.idle": "2026-01-20T01:29:07.615689Z",
     "shell.execute_reply": "2026-01-20T01:29:07.615416Z"
    },
    "papermill": {
     "duration": 0.013447,
     "end_time": "2026-01-20T01:29:07.616290",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.602843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dino_xcit_small_12_p8</td>\n",
       "      <td>Dino-XCIT-S12-P8</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>xcit_small_12_p8</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-XCIT-S12-P8 trained via self supervision ...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dino_vits8</td>\n",
       "      <td>Dino-VIT-S8</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>vits8</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-VIT-S8 trained via self supervision with ...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dino_resnet50</td>\n",
       "      <td>Dino-ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-ResNet50 trained via self supervision wit...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model model_display_name      train_type      architecture  \\\n",
       "5  dino_xcit_small_12_p8   Dino-XCIT-S12-P8  selfsupervised  xcit_small_12_p8   \n",
       "1             dino_vits8        Dino-VIT-S8  selfsupervised             vits8   \n",
       "8          dino_resnet50      Dino-ResNet50  selfsupervised          resnet50   \n",
       "\n",
       "     model_class train_data  \\\n",
       "5    Transformer   imagenet   \n",
       "1    Transformer   imagenet   \n",
       "8  Convolutional   imagenet   \n",
       "\n",
       "                                         description    task_cluster  \\\n",
       "5  Dino-XCIT-S12-P8 trained via self supervision ...  SelfSupervised   \n",
       "1  Dino-VIT-S8 trained via self supervision with ...  SelfSupervised   \n",
       "8  Dino-ResNet50 trained via self supervision wit...  SelfSupervised   \n",
       "\n",
       "  model_source                          model_source_url  \n",
       "5         dino  https://github.com/facebookresearch/dino  \n",
       "1         dino  https://github.com/facebookresearch/dino  \n",
       "8         dino  https://github.com/facebookresearch/dino  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01f215",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.00313,
     "end_time": "2026-01-20T01:29:07.623001",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.619871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MiDas Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196c9faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.630751Z",
     "iopub.status.busy": "2026-01-20T01:29:07.630651Z",
     "iopub.status.idle": "2026-01-20T01:29:07.639689Z",
     "shell.execute_reply": "2026-01-20T01:29:07.639430Z"
    },
    "papermill": {
     "duration": 0.013929,
     "end_time": "2026-01-20T01:29:07.640752",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.626823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "midas_dictlist = [\n",
    "    {'model': 'DPT_Hybrid', 'model_display_name': 'DPT-Hybrid'},\n",
    "    {'model': 'DPT_Large', 'model_display_name': 'DPT-Large'},\n",
    "    {'model': 'MiDaS', 'model_display_name': 'MiDaS'},\n",
    "    {'model': 'MiDaS_small', 'model_display_name': 'MiDaS-S'} \n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(midas_dictlist):\n",
    "    dict_i = midas_dictlist[i]\n",
    "    dict_i['description'] = '{}, a monocular depth estimation model.'.format(dict_i['model_display_name'])\n",
    "    if 'DPT' in dict_i['model']:\n",
    "        dict_i['model_class'] = 'Transformer'\n",
    "    if 'MiDas' in dict_i['model']:\n",
    "        dict_i['model_class'] = 'Convolutional'\n",
    "    \n",
    "midas_df = pd.DataFrame(midas_dictlist)\n",
    "midas_df['architecture'] = midas_df['model']\n",
    "midas_df['train_type'] = 'monoculardepth'\n",
    "midas_df['train_data'] = 'ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HRWSI,ApolloScape,BlendedMVS,IRS'\n",
    "midas_df['task_cluster'] = 'MonocularDepth'\n",
    "midas_df['model_source'] = 'midas'\n",
    "midas_df['model_source_url'] = 'https://github.com/isl-org/MiDaS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be71f604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.648277Z",
     "iopub.status.busy": "2026-01-20T01:29:07.648123Z",
     "iopub.status.idle": "2026-01-20T01:29:07.657875Z",
     "shell.execute_reply": "2026-01-20T01:29:07.657639Z"
    },
    "papermill": {
     "duration": 0.014165,
     "end_time": "2026-01-20T01:29:07.658608",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.644443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>model_class</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MiDaS_small</td>\n",
       "      <td>MiDaS-S</td>\n",
       "      <td>MiDaS-S, a monocular depth estimation model.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MiDaS_small</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPT_Hybrid</td>\n",
       "      <td>DPT-Hybrid</td>\n",
       "      <td>DPT-Hybrid, a monocular depth estimation model.</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>DPT_Hybrid</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPT_Large</td>\n",
       "      <td>DPT-Large</td>\n",
       "      <td>DPT-Large, a monocular depth estimation model.</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>DPT_Large</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model model_display_name  \\\n",
       "3  MiDaS_small            MiDaS-S   \n",
       "0   DPT_Hybrid         DPT-Hybrid   \n",
       "1    DPT_Large          DPT-Large   \n",
       "\n",
       "                                       description  model_class architecture  \\\n",
       "3     MiDaS-S, a monocular depth estimation model.          NaN  MiDaS_small   \n",
       "0  DPT-Hybrid, a monocular depth estimation model.  Transformer   DPT_Hybrid   \n",
       "1   DPT-Large, a monocular depth estimation model.  Transformer    DPT_Large   \n",
       "\n",
       "       train_type                                         train_data  \\\n",
       "3  monoculardepth  ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...   \n",
       "0  monoculardepth  ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...   \n",
       "1  monoculardepth  ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...   \n",
       "\n",
       "     task_cluster model_source                  model_source_url  \n",
       "3  MonocularDepth        midas  https://github.com/isl-org/MiDaS  \n",
       "0  MonocularDepth        midas  https://github.com/isl-org/MiDaS  \n",
       "1  MonocularDepth        midas  https://github.com/isl-org/MiDaS  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midas_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67adb104",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003098,
     "end_time": "2026-01-20T01:29:07.665602",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.662504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### YoloV5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a2d61fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.673641Z",
     "iopub.status.busy": "2026-01-20T01:29:07.673375Z",
     "iopub.status.idle": "2026-01-20T01:29:07.683040Z",
     "shell.execute_reply": "2026-01-20T01:29:07.682765Z"
    },
    "papermill": {
     "duration": 0.014466,
     "end_time": "2026-01-20T01:29:07.683827",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.669361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_dictlist = [\n",
    "    {'model': 'yolov5l', 'model_display_name': 'YOLO-V5-L'},\n",
    "    {'model': 'yolov5m', 'model_display_name': 'YOLO-V5-M'},\n",
    "    {'model': 'yolov5n', 'model_display_name': 'YOLO-V5-N'},\n",
    "    {'model': 'yolov5s', 'model_display_name': 'YOLO-V5-S'},\n",
    "    {'model': 'yolov5x', 'model_display_name': 'YOLO-V5-S'},\n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(yolo_dictlist):\n",
    "    dict_i = yolo_dictlist[i]\n",
    "    dict_i['description'] = '{}, an object detection model.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "yolo_df = pd.DataFrame(yolo_dictlist)\n",
    "yolo_df['architecture'] = yolo_df['model']\n",
    "yolo_df['model_class'] = 'Convolutional'\n",
    "yolo_df['train_type'] = 'yolo'\n",
    "yolo_df['train_data'] = 'coco,voc'\n",
    "yolo_df['task_cluster'] = 'Detection'\n",
    "yolo_df['model_source'] = 'yolo'\n",
    "yolo_df['model_source_url'] = 'https://github.com/ultralytics/yolov5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54560fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.691594Z",
     "iopub.status.busy": "2026-01-20T01:29:07.691402Z",
     "iopub.status.idle": "2026-01-20T01:29:07.702018Z",
     "shell.execute_reply": "2026-01-20T01:29:07.701771Z"
    },
    "papermill": {
     "duration": 0.015557,
     "end_time": "2026-01-20T01:29:07.702886",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.687329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yolov5x</td>\n",
       "      <td>YOLO-V5-S</td>\n",
       "      <td>YOLO-V5-S, an object detection model.</td>\n",
       "      <td>yolov5x</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>yolo</td>\n",
       "      <td>coco,voc</td>\n",
       "      <td>Detection</td>\n",
       "      <td>yolo</td>\n",
       "      <td>https://github.com/ultralytics/yolov5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model model_display_name                            description  \\\n",
       "4  yolov5x          YOLO-V5-S  YOLO-V5-S, an object detection model.   \n",
       "\n",
       "  architecture    model_class train_type train_data task_cluster model_source  \\\n",
       "4      yolov5x  Convolutional       yolo   coco,voc    Detection         yolo   \n",
       "\n",
       "                        model_source_url  \n",
       "4  https://github.com/ultralytics/yolov5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b099c0",
   "metadata": {
    "papermill": {
     "duration": 0.003341,
     "end_time": "2026-01-20T01:29:07.710321",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.706980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Detectron Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4522408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.717618Z",
     "iopub.status.busy": "2026-01-20T01:29:07.717451Z",
     "iopub.status.idle": "2026-01-20T01:29:07.728958Z",
     "shell.execute_reply": "2026-01-20T01:29:07.728673Z"
    },
    "papermill": {
     "duration": 0.016036,
     "end_time": "2026-01-20T01:29:07.729739",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.713703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "detectron_dictlist = [\n",
    "    {'model': 'faster_rcnn_R_50_C4_3x', 'model_display_name': 'Faster-RCNN-ResNet50-C4',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_C4_3x.yaml'},\n",
    "        \n",
    "    {'model': 'faster_rcnn_R_50_DC5_3x', 'model_display_name': 'Faster-RCNN-ResNet50-DC5',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_50_FPN_3x', 'model_display_name': 'Faster-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_101_C4_3x', 'model_display_name': 'Faster-RCNN-ResNet101-C4',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_C4_3x.yaml'},\n",
    "        \n",
    "    {'model': 'faster_rcnn_R_101_DC5_3x', 'model_display_name': 'Faster-RCNN-ResNet101-DC5',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_101_FPN_3x', 'model_display_name': 'Faster-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_X_101_32x8d_FPN_3x', 'model_display_name': 'Faster-RCNN-X101-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'retinanet_R_50_FPN_3x', 'model_display_name': 'RetinaNet-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Detection/retinanet_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'retinanet_R_101_FPN_3x', 'model_display_name': 'RetinaNet-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-Detection/retinanet_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_C4_3x', 'model_display_name': 'Mask-RCNN-ResNet50-C4',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_DC5_3x', 'model_display_name': 'Mask-RCNN-ResNet50-DC5',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_FPN_3x', 'model_display_name': 'Mask-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_C4_3x', 'model_display_name': 'Mask-RCNN-ResNet101-C4',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_DC5_3x', 'model_display_name': 'Mask-RCNN-ResNet101-DC5',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_FPN_3x', 'model_display_name': 'Mask-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_X_101_32x8d_FPN_3x', 'model_display_name': 'Mask-RCNN-X101-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml'},\n",
    "        \n",
    "    {'model': 'keypoint_rcnn_R_50_FPN_3x', 'model_display_name': 'Keypoint-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'panoptic_fpn_R_50_3x', 'model_display_name': 'Panoptic-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_FPN_1x', 'model_display_name': 'LVIS-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_FPN_1x', 'model_display_name': 'LVIS-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_X_101_32x8d_FPN_1x', 'model_display_name': 'LVIS-RCNN-X101-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml'},\n",
    "    \n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(detectron_dictlist):\n",
    "    dict_i = detectron_dictlist[i]\n",
    "    train_data = 'coco2017'\n",
    "    if 'COCO-Detection' in dict_i['weights_url']:\n",
    "        train_type = 'detection'\n",
    "    if 'COCO-InstanceSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "    if 'new_baselines' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "    if 'CoCo-Keypoints' in dict_i['weights_url']:\n",
    "        train_type = 'keypoints'\n",
    "    if 'COCO-PanopticSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'panoptics'\n",
    "    if 'COCO-PanopticSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'panoptics'\n",
    "    if 'LVISv0.5-InstanceSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "        train_data = 'LVIS'\n",
    "    dict_i['train_type'] = train_type\n",
    "    dict_i['train_data'] = train_data\n",
    "    dict_i['description'] = '{}, trained on {} with the CoCo2017 dataset.'.format(dict_i['model_display_name'], train_type)\n",
    "    \n",
    "detectron_df = pd.DataFrame(detectron_dictlist)\n",
    "detectron_df['architecture'] = detectron_df['model']\n",
    "detectron_df['task_cluster'] = 'Detection|Segmentation'\n",
    "detectron_df['model_class'] = 'Convolutional'\n",
    "detectron_df['model_source'] = 'detectron'\n",
    "detectron_df['model_source_url'] = 'https://github.com/facebookresearch/detectron2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6f5849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.738234Z",
     "iopub.status.busy": "2026-01-20T01:29:07.738111Z",
     "iopub.status.idle": "2026-01-20T01:29:07.747995Z",
     "shell.execute_reply": "2026-01-20T01:29:07.747760Z"
    },
    "papermill": {
     "duration": 0.01502,
     "end_time": "2026-01-20T01:29:07.748705",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.733685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>weights_url</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faster_rcnn_R_101_DC5_3x</td>\n",
       "      <td>Faster-RCNN-ResNet101-DC5</td>\n",
       "      <td>COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml</td>\n",
       "      <td>detection</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Faster-RCNN-ResNet101-DC5, trained on detectio...</td>\n",
       "      <td>faster_rcnn_R_101_DC5_3x</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faster_rcnn_R_50_DC5_3x</td>\n",
       "      <td>Faster-RCNN-ResNet50-DC5</td>\n",
       "      <td>COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml</td>\n",
       "      <td>detection</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Faster-RCNN-ResNet50-DC5, trained on detection...</td>\n",
       "      <td>faster_rcnn_R_50_DC5_3x</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mask_rcnn_R_50_FPN_3x</td>\n",
       "      <td>Mask-RCNN-ResNet50-FPN</td>\n",
       "      <td>COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3...</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Mask-RCNN-ResNet50-FPN, trained on segmentatio...</td>\n",
       "      <td>mask_rcnn_R_50_FPN_3x</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model         model_display_name  \\\n",
       "4   faster_rcnn_R_101_DC5_3x  Faster-RCNN-ResNet101-DC5   \n",
       "1    faster_rcnn_R_50_DC5_3x   Faster-RCNN-ResNet50-DC5   \n",
       "11     mask_rcnn_R_50_FPN_3x     Mask-RCNN-ResNet50-FPN   \n",
       "\n",
       "                                          weights_url    train_type  \\\n",
       "4        COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml     detection   \n",
       "1         COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml     detection   \n",
       "11  COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3...  segmentation   \n",
       "\n",
       "   train_data                                        description  \\\n",
       "4    coco2017  Faster-RCNN-ResNet101-DC5, trained on detectio...   \n",
       "1    coco2017  Faster-RCNN-ResNet50-DC5, trained on detection...   \n",
       "11   coco2017  Mask-RCNN-ResNet50-FPN, trained on segmentatio...   \n",
       "\n",
       "                architecture            task_cluster    model_class  \\\n",
       "4   faster_rcnn_R_101_DC5_3x  Detection|Segmentation  Convolutional   \n",
       "1    faster_rcnn_R_50_DC5_3x  Detection|Segmentation  Convolutional   \n",
       "11     mask_rcnn_R_50_FPN_3x  Detection|Segmentation  Convolutional   \n",
       "\n",
       "   model_source                                 model_source_url  \n",
       "4     detectron  https://github.com/facebookresearch/detectron2/  \n",
       "1     detectron  https://github.com/facebookresearch/detectron2/  \n",
       "11    detectron  https://github.com/facebookresearch/detectron2/  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectron_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717c91d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003418,
     "end_time": "2026-01-20T01:29:07.755866",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.752448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Timm (+Pretrain) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f0451f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.763903Z",
     "iopub.status.busy": "2026-01-20T01:29:07.763755Z",
     "iopub.status.idle": "2026-01-20T01:29:07.779062Z",
     "shell.execute_reply": "2026-01-20T01:29:07.778784Z"
    },
    "papermill": {
     "duration": 0.020034,
     "end_time": "2026-01-20T01:29:07.779880",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.759846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timm_pluspretrain_dictlist = [\n",
    "    \n",
    "    {'model': 'adv_inception_v3', 'model_display_name': 'Adversarial-Inception-V3', \n",
    "     'architecture': 'inception_v3',\n",
    "     'train_type': 'adversarial', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'beit_base_patch16_224', 'model_display_name': 'BEiT-S-P16',\n",
    "     'architecture': 'beit_base_patch16_224',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'beit_large_patch16_224', 'model_display_name': 'BEiT-L-P16',\n",
    "     'architecture': 'beit_large_patch16_224',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'resnetv2_50x1_bitm', 'model_display_name': 'ResNetV2-50x1-BitM', \n",
    "     'architecture': 'resnetv2_50x1',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_101x3_bitm', 'model_display_name': 'ResNetV2-101x3-BitM',\n",
    "     'architecture': 'resnetv2_101x3',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_152x4_bitm', 'model_display_name': 'ResNetV2-152x4-BitM',\n",
    "     'architecture': 'resnetv2_152x4',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_50x1_bitm_in21k', 'model_display_name': 'ResNetV2-50x1-BitM-IN21K', \n",
    "     'architecture': 'resnetv2_50x1',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_101x3_bitm_in21k', 'model_display_name': 'ResNetV2-101x3-BitM-IN21K', \n",
    "     'architecture': 'resnetv2_101x3',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_152x4_bitm_in21k', 'model_display_name': 'ResNetV2-152x4-BitM-IN21K', \n",
    "     'architecture': 'resnetv2_152x4',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_base_in22k', 'model_display_name': 'ConvNext-Base-IN21K', \n",
    "     'architecture': 'convnext_base',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_large_in22k', 'model_display_name': 'ConvNext-Large-IN21K', \n",
    "     'architecture': 'convnext_large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "\n",
    "    {'model': 'mixer_b16_224_in21k', 'model_display_name': 'Mixer-B16-IN22K',\n",
    "     'architecture': 'mixer_b16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'mixer_l16_224_in21k', 'model_display_name': 'Mixer-L16-IN22K',\n",
    "     'architecture': 'mixer_l16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_large_100_miil_in21k', 'model_display_name': 'MobileNet-V3-Large-IN21K', \n",
    "     'architecture': 'mobilenetv3_large_100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window7_224_in22k', 'model_display_name': 'Swin-B-P4-W7-IN21K', \n",
    "     'architecture': 'swin_base_patch4_window7_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window7_224_in22k', 'model_display_name': 'Swin-L-P4-W7-IN21K', \n",
    "     'architecture': 'swin_large_patch4_window7_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'resmlp_big_24_224_in22ft1k', 'model_display_name': 'ResMLP-Big-24-IN21K', \n",
    "     'architecture': 'resmlp_big_24_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'vit_base_r50_s16_224_in21k', 'model_display_name': 'ViT-B-R50-S16-IN21K', \n",
    "     'architecture': 'vit_base_r50_s16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224_in21k', 'model_display_name': 'ViT-B-P16-IN21K', \n",
    "     'architecture': 'vit_base_patch16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_224_in21k', 'model_display_name': 'ViT-S-P16-IN21K', \n",
    "     'architecture': 'vit_small_patch16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_224_in21k', 'model_display_name': 'ViT-L-P16-IN21K', \n",
    "     'architecture': 'vit_large_patch16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_224_in21k', 'model_display_name': 'ViT-B-P32-IN21K', \n",
    "     'architecture': 'vit_base_patch32_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_224_in21k', 'model_display_name': 'ViT-S-P32-IN21K', \n",
    "     'architecture': 'vit_small_patch32_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch32_224_in21k', 'model_display_name': 'ViT-L-P32-IN21K', \n",
    "     'architecture': 'vit_large_patch32_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ssl_resnet18', 'model_display_name': 'ResNet18-SSL',\n",
    "     'architecture': 'resnet18',\n",
    "     'train_type': 'semi-supervised', 'train_data': 'YFCC100M', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ssl_resnet50', 'model_display_name': 'ResNet50-SSL',\n",
    "     'architecture': 'resnet50',\n",
    "     'train_type': 'semi-supervised', 'train_data': 'YFCC100M', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ssl_resnext101_32x4d', 'model_display_name': 'ResNext101-32x4D-SSL', \n",
    "     'architecture': 'resnext101_32x4d',\n",
    "     'train_type': 'semi-supervised', 'train_data': 'YFCC100M', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swsl_resnet18', 'model_display_name': 'Resnet18-SWSL', \n",
    "     'architecture': 'resnet18',\n",
    "     'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swsl_resnet50', 'model_display_name': 'Resnet50-SWSL',\n",
    "     'architecture': 'resnet50',\n",
    "     'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swsl_resnext101_32x4d', 'model_display_name': 'ResNext101-32x4D-SWSL',\n",
    "     'architecture': 'resnext101_32x4d',\n",
    "     'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_sam_224', 'model_display_name': 'ViT-B-P16-SAM', \n",
    "     'architecture': 'vit_base_patch16_224',\n",
    "     'train_type': 'sam_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_sam_224', 'model_display_name': 'ViT-S-P32-SAM', \n",
    "     'architecture': 'vit_base_patch16_224',\n",
    "     'train_type': 'sam_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "\n",
    "]\n",
    "\n",
    "tf_additions = []\n",
    "for depth in range(8):\n",
    "    for train_type_index in range(3):\n",
    "        train_type_codex = ['','_ap','_ns'][train_type_index]\n",
    "        train_type_title = ['','AP','NS'][train_type_index]\n",
    "        train_type_label = ['classification','adversarial','noisy_student'][train_type_index]\n",
    "        tf_additions.append({'model': 'tf_efficientnet_b{}{}'.format(depth, train_type_codex),\n",
    "                             'architecture': 'tf_efficientnet_b{}'.format(depth),\n",
    "                             'model_display_name': 'TF-EfficientNet-B{}{}'.format(depth, train_type_title),\n",
    "                             'train_type': train_type_label, 'train_data': 'imagenet', 'model_class': 'Convolutional'})\n",
    "        \n",
    "timm_pluspretrain_dictlist += tf_additions\n",
    "\n",
    "train_type_text = {\n",
    "    'classification': 'image classification',\n",
    "    'bert_pretraining': 'masked-input-pretrained classification',\n",
    "    'adversarial': 'adversarial classification',\n",
    "    'noisy_student': 'noisy student classification',\n",
    "    'big_transfer': 'large dataset pretraining',\n",
    "    'sam_pretraining': 'sharpness-aware pretraining',\n",
    "    'semi-supervised': 'semi-supervised image classification',\n",
    "    'semi-weakly-supervised': 'semi-weakly-supervised image classification',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'imagenet21k': 'ImageNet21K',\n",
    "    'big_transfer': 'BiT-M',\n",
    "    'YFCC100M': 'YFCC100M',\n",
    "    'Instagram': 'Instagram', \n",
    "}\n",
    "\n",
    "for i, dict_i in enumerate(timm_pluspretrain_dictlist):\n",
    "    dict_i = timm_pluspretrain_dictlist[i]\n",
    "    dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']],\n",
    "                                                                           train_data_text[dict_i['train_data']])\n",
    "        \n",
    "timm_pluspretrain_df = pd.DataFrame(timm_pluspretrain_dictlist)\n",
    "timm_pluspretrain_df['task_cluster'] = 'Semantic'\n",
    "timm_pluspretrain_df['model_source'] = 'timm'\n",
    "timm_pluspretrain_df['model_source_url'] = 'https://github.com/rwightman/pytorch-image-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "010533ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.787900Z",
     "iopub.status.busy": "2026-01-20T01:29:07.787777Z",
     "iopub.status.idle": "2026-01-20T01:29:07.797285Z",
     "shell.execute_reply": "2026-01-20T01:29:07.797001Z"
    },
    "papermill": {
     "duration": 0.014018,
     "end_time": "2026-01-20T01:29:07.797917",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.783899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vit_base_patch32_224_in21k</td>\n",
       "      <td>ViT-B-P32-IN21K</td>\n",
       "      <td>vit_base_patch32_224</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet21k</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>ViT-B-P32-IN21K trained on image classificatio...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adv_inception_v3</td>\n",
       "      <td>Adversarial-Inception-V3</td>\n",
       "      <td>inception_v3</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Adversarial-Inception-V3 trained on adversaria...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beit_base_patch16_224</td>\n",
       "      <td>BEiT-S-P16</td>\n",
       "      <td>beit_base_patch16_224</td>\n",
       "      <td>bert_pretraining</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>BEiT-S-P16 trained on masked-input-pretrained ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model        model_display_name  \\\n",
       "21  vit_base_patch32_224_in21k           ViT-B-P32-IN21K   \n",
       "0             adv_inception_v3  Adversarial-Inception-V3   \n",
       "1        beit_base_patch16_224                BEiT-S-P16   \n",
       "\n",
       "             architecture        train_type   train_data    model_class  \\\n",
       "21   vit_base_patch32_224    classification  imagenet21k    Transformer   \n",
       "0            inception_v3       adversarial     imagenet  Convolutional   \n",
       "1   beit_base_patch16_224  bert_pretraining     imagenet    Transformer   \n",
       "\n",
       "                                          description task_cluster  \\\n",
       "21  ViT-B-P32-IN21K trained on image classificatio...     Semantic   \n",
       "0   Adversarial-Inception-V3 trained on adversaria...     Semantic   \n",
       "1   BEiT-S-P16 trained on masked-input-pretrained ...     Semantic   \n",
       "\n",
       "   model_source                                   model_source_url  \n",
       "21         timm  https://github.com/rwightman/pytorch-image-mod...  \n",
       "0          timm  https://github.com/rwightman/pytorch-image-mod...  \n",
       "1          timm  https://github.com/rwightman/pytorch-image-mod...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_pluspretrain_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8b29f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003433,
     "end_time": "2026-01-20T01:29:07.805042",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.801609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Timm (+Resolution) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d856e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.812872Z",
     "iopub.status.busy": "2026-01-20T01:29:07.812758Z",
     "iopub.status.idle": "2026-01-20T01:29:07.822228Z",
     "shell.execute_reply": "2026-01-20T01:29:07.821871Z"
    },
    "papermill": {
     "duration": 0.014226,
     "end_time": "2026-01-20T01:29:07.823011",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.808785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timm_plusres_dictlist = [\n",
    "    \n",
    "    {'model': 'beit_base_patch16_384', 'model_display_name': 'BEiT-B-P16',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'beit_base_patch16_384', 'model_display_name': 'BEiT-L-P16',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'beit_base_patch16_512', 'model_display_name': 'BEiT-L-P16',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_s24_384', 'model_display_name': 'CaiT-S-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_s24_384', 'model_display_name': 'CaiT-S-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'convnext_base_384_in22ft1k', 'model_display_name': 'ConvNext-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_large_384_in22ft1k', 'model_display_name': 'ConvNext-L',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'crossvit_15_dagger_408', 'model_display_name': 'CrossViT-18-Dagger',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_base_patch16_384', 'model_display_name': 'Deit-B-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'levit_384', 'model_display_name': 'LeViT',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window12_384', 'model_display_name': 'Swin-B-P4-W12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window12_384', 'model_display_name': 'Swin-L-P4-W12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window12_384', 'model_display_name': 'Swin-L-P4-W12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_384', 'model_display_name': 'ViT-B-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_384', 'model_display_name': 'ViT-B-P32',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_384', 'model_display_name': 'ViT-L-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch32_384', 'model_display_name': 'ViT-L-P32',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_384', 'model_display_name': 'ViT-S-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_384', 'model_display_name': 'ViT-S-P32',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d1_384', 'model_display_name': 'VoLo-D1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa06d56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.0035,
     "end_time": "2026-01-20T01:29:07.830071",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.826571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91aec69f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.838071Z",
     "iopub.status.busy": "2026-01-20T01:29:07.837946Z",
     "iopub.status.idle": "2026-01-20T01:29:07.847861Z",
     "shell.execute_reply": "2026-01-20T01:29:07.847524Z"
    },
    "papermill": {
     "duration": 0.014422,
     "end_time": "2026-01-20T01:29:07.848552",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.834130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "slip_dictlist = [\n",
    "    \n",
    "    {'model': 'ViT-S-SimCLR', 'train_type': 'SimCLR'},\n",
    "    {'model': 'ViT-S-CLIP', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-S-SLIP', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-B-SimCLR', 'train_type': 'SimCLR'},\n",
    "    {'model': 'ViT-B-CLIP', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-B-SLIP', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-L-SimCLR', 'train_type': 'SimCLR'},\n",
    "    {'model': 'ViT-L-CLIP', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-L-SLIP', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-S-SLIP-Ep100', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-B-SLIP-Ep100', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-L-SLIP-Ep100', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-L-CLIP-CC12M', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-L-SLIP-CC12M', 'train_type': 'SLIP'},  \n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'SimCLR': 'pure self-supervision',\n",
    "    'CLIP': 'pure language supervision',\n",
    "    'SLIP': 'combined self- and language supervision',\n",
    "}\n",
    "    \n",
    "for i, dict_i in enumerate(slip_dictlist):\n",
    "    dict_i = slip_dictlist[i]\n",
    "    dict_i['architecture'] = '-'.join(dict_i['model'].split('-')[:2])\n",
    "    dict_i['model_display_name'] = dict_i['model']\n",
    "    dict_i['model_class'] = 'Transformer'\n",
    "    if 'SimCLR' in dict_i['model']:\n",
    "        dict_i['task_cluster'] = 'SelfSupervised'\n",
    "    if 'CLIP' in dict_i['model']:\n",
    "        dict_i['task_cluster'] = 'Vision-Language'\n",
    "    if 'SLIP' in dict_i['model']:\n",
    "        dict_i['task_cluster'] = 'Vision-Language'\n",
    "    dict_i['train_data'] = 'YFCC15M'\n",
    "    dict_i['description'] = '{} trained via {} with the YFCC15M dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']])\n",
    "    dict_i['train_type'] = 'slip'\n",
    "    \n",
    "slip_df = pd.DataFrame(slip_dictlist)\n",
    "slip_df['model_source'] = 'slip'\n",
    "slip_df['model_source_url'] = 'https://github.com/facebookresearch/slip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06f89b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.855951Z",
     "iopub.status.busy": "2026-01-20T01:29:07.855813Z",
     "iopub.status.idle": "2026-01-20T01:29:07.864782Z",
     "shell.execute_reply": "2026-01-20T01:29:07.864569Z"
    },
    "papermill": {
     "duration": 0.013163,
     "end_time": "2026-01-20T01:29:07.865394",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.852231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ViT-S-SLIP</td>\n",
       "      <td>slip</td>\n",
       "      <td>ViT-S</td>\n",
       "      <td>ViT-S-SLIP</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>YFCC15M</td>\n",
       "      <td>ViT-S-SLIP trained via combined self- and lang...</td>\n",
       "      <td>slip</td>\n",
       "      <td>https://github.com/facebookresearch/slip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ViT-S-SLIP-Ep100</td>\n",
       "      <td>slip</td>\n",
       "      <td>ViT-S</td>\n",
       "      <td>ViT-S-SLIP-Ep100</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>YFCC15M</td>\n",
       "      <td>ViT-S-SLIP-Ep100 trained via combined self- an...</td>\n",
       "      <td>slip</td>\n",
       "      <td>https://github.com/facebookresearch/slip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ViT-L-SLIP-Ep100</td>\n",
       "      <td>slip</td>\n",
       "      <td>ViT-L</td>\n",
       "      <td>ViT-L-SLIP-Ep100</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>YFCC15M</td>\n",
       "      <td>ViT-L-SLIP-Ep100 trained via combined self- an...</td>\n",
       "      <td>slip</td>\n",
       "      <td>https://github.com/facebookresearch/slip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model train_type architecture model_display_name  model_class  \\\n",
       "2         ViT-S-SLIP       slip        ViT-S         ViT-S-SLIP  Transformer   \n",
       "9   ViT-S-SLIP-Ep100       slip        ViT-S   ViT-S-SLIP-Ep100  Transformer   \n",
       "11  ViT-L-SLIP-Ep100       slip        ViT-L   ViT-L-SLIP-Ep100  Transformer   \n",
       "\n",
       "       task_cluster train_data  \\\n",
       "2   Vision-Language    YFCC15M   \n",
       "9   Vision-Language    YFCC15M   \n",
       "11  Vision-Language    YFCC15M   \n",
       "\n",
       "                                          description model_source  \\\n",
       "2   ViT-S-SLIP trained via combined self- and lang...         slip   \n",
       "9   ViT-S-SLIP-Ep100 trained via combined self- an...         slip   \n",
       "11  ViT-L-SLIP-Ep100 trained via combined self- an...         slip   \n",
       "\n",
       "                            model_source_url  \n",
       "2   https://github.com/facebookresearch/slip  \n",
       "9   https://github.com/facebookresearch/slip  \n",
       "11  https://github.com/facebookresearch/slip  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slip_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0265f971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003302,
     "end_time": "2026-01-20T01:29:07.872405",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.869103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SEER Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63f97296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.880390Z",
     "iopub.status.busy": "2026-01-20T01:29:07.880276Z",
     "iopub.status.idle": "2026-01-20T01:29:07.888617Z",
     "shell.execute_reply": "2026-01-20T01:29:07.888346Z"
    },
    "papermill": {
     "duration": 0.013009,
     "end_time": "2026-01-20T01:29:07.889302",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.876293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seer_model_list = [\n",
    "    'RegNet-32Gf-SEER',\n",
    "    'RegNet-32Gf-SEER-INFT',\n",
    "    'RegNet-64Gf-SEER',\n",
    "    'RegNet-64Gf-SEER-INFT',\n",
    "    'RegNet-128Gf-SEER',\n",
    "    'RegNet-128Gf-SEER-INFT',\n",
    "    'RegNet-256Gf-SEER',\n",
    "    'RegNet-256Gf-SEER-INFT',\n",
    "]\n",
    "\n",
    "seer_dictlist = []\n",
    "for model in seer_model_list:\n",
    "    dict_i = {'model': model, 'model_display_name': model, 'train_type': 'seer', 'train_data': 'random1B'}\n",
    "    dict_i['architecture'] = '-'.join(model.split('-')[:2])\n",
    "    dict_i['model_class'] = 'Convolutional'\n",
    "    dict_i['description'] = ('{} trained via large-scale self-supervision on 1 billion images.'\n",
    "                             .format(dict_i['architecture']))\n",
    "                             \n",
    "    seer_dictlist.append(dict_i)\n",
    "\n",
    "seer_df = pd.DataFrame(seer_dictlist)\n",
    "seer_df['task_cluster'] = 'SelfSupervised'\n",
    "seer_df['model_source'] = 'seer'\n",
    "seer_df['model_source_url'] = 'https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10a9d1c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.897231Z",
     "iopub.status.busy": "2026-01-20T01:29:07.897122Z",
     "iopub.status.idle": "2026-01-20T01:29:07.906569Z",
     "shell.execute_reply": "2026-01-20T01:29:07.906306Z"
    },
    "papermill": {
     "duration": 0.014117,
     "end_time": "2026-01-20T01:29:07.907224",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.893107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RegNet-64Gf-SEER</td>\n",
       "      <td>RegNet-64Gf-SEER</td>\n",
       "      <td>seer</td>\n",
       "      <td>random1B</td>\n",
       "      <td>RegNet-64Gf</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>RegNet-64Gf trained via large-scale self-super...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>seer</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/blob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RegNet-64Gf-SEER-INFT</td>\n",
       "      <td>RegNet-64Gf-SEER-INFT</td>\n",
       "      <td>seer</td>\n",
       "      <td>random1B</td>\n",
       "      <td>RegNet-64Gf</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>RegNet-64Gf trained via large-scale self-super...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>seer</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/blob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RegNet-32Gf-SEER</td>\n",
       "      <td>RegNet-32Gf-SEER</td>\n",
       "      <td>seer</td>\n",
       "      <td>random1B</td>\n",
       "      <td>RegNet-32Gf</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>RegNet-32Gf trained via large-scale self-super...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>seer</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/blob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model     model_display_name train_type train_data  \\\n",
       "2       RegNet-64Gf-SEER       RegNet-64Gf-SEER       seer   random1B   \n",
       "3  RegNet-64Gf-SEER-INFT  RegNet-64Gf-SEER-INFT       seer   random1B   \n",
       "0       RegNet-32Gf-SEER       RegNet-32Gf-SEER       seer   random1B   \n",
       "\n",
       "  architecture    model_class  \\\n",
       "2  RegNet-64Gf  Convolutional   \n",
       "3  RegNet-64Gf  Convolutional   \n",
       "0  RegNet-32Gf  Convolutional   \n",
       "\n",
       "                                         description    task_cluster  \\\n",
       "2  RegNet-64Gf trained via large-scale self-super...  SelfSupervised   \n",
       "3  RegNet-64Gf trained via large-scale self-super...  SelfSupervised   \n",
       "0  RegNet-32Gf trained via large-scale self-super...  SelfSupervised   \n",
       "\n",
       "  model_source                                   model_source_url  \n",
       "2         seer  https://github.com/facebookresearch/vissl/blob...  \n",
       "3         seer  https://github.com/facebookresearch/vissl/blob...  \n",
       "0         seer  https://github.com/facebookresearch/vissl/blob...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seer_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b5d9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003201,
     "end_time": "2026-01-20T01:29:07.914229",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.911028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### IPCL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19de05e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.922162Z",
     "iopub.status.busy": "2026-01-20T01:29:07.922056Z",
     "iopub.status.idle": "2026-01-20T01:29:07.930800Z",
     "shell.execute_reply": "2026-01-20T01:29:07.930535Z"
    },
    "papermill": {
     "duration": 0.013497,
     "end_time": "2026-01-20T01:29:07.931567",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.918070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipcl_options = {'alexnet_gn_ipcl_imagenet': 'ipcl1',\n",
    "                'alexnet_gn_ipcl_openimages': 'ipcl7',\n",
    "                'alexnet_gn_ipcl_places256': 'ipcl8',\n",
    "                'alexnet_gn_ipcl_vggface2': 'ipcl9',\n",
    "                'alexnet_gn_ipcl_random': 'ipcl16'}\n",
    "\n",
    "train_type_text = {\n",
    "    'ipcl': 'IPCL-Style-Self-Supervision',\n",
    "    'catsup': 'Category-Supervision',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'openimages': 'OpenImages',\n",
    "    'places256': 'Places256',\n",
    "    'vggface2': 'VGGFace2',\n",
    "}\n",
    "\n",
    "ipcl_dictlist = []\n",
    "for ipcl_option in ipcl_options:\n",
    "    if 'random' not in ipcl_option:\n",
    "        dict_i = {'model': ipcl_option, 'train_type': ipcl_option.split('_')[-2], \n",
    "                  'train_data': ipcl_option.split('_')[-1]}\n",
    "        dict_i['model_display_name'] = ('AlexNet-GN-' + dict_i['train_type'].upper() \n",
    "                                        + train_data_text[dict_i['train_data']])\n",
    "        dict_i['description'] = ('AlexNet (modified with GroupNorm) trained via {} on {}.'\n",
    "                                 .format(train_type_text[dict_i['train_type']],\n",
    "                                         train_data_text[dict_i['train_data']]))\n",
    "        dict_i['TaskCluster'] = 'SelfSupervised' if dict_i['train_type'] == 'ipcl' else 'SelfSupervised'\n",
    "        \n",
    "        ipcl_dictlist.append(dict_i)\n",
    "        \n",
    "ipcl_df = pd.DataFrame(ipcl_dictlist)\n",
    "ipcl_df['architecture'] = 'alexnet_gn'\n",
    "ipcl_df['model_class'] = 'Convolutional'\n",
    "ipcl_df['model_source'] = 'open_ipcl'\n",
    "ipcl_df['model_source_url'] = 'https://github.com/harvard-visionlab/open_ipcl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e3188",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003344,
     "end_time": "2026-01-20T01:29:07.938461",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.935117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### BIT Expert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9bee060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.947014Z",
     "iopub.status.busy": "2026-01-20T01:29:07.946894Z",
     "iopub.status.idle": "2026-01-20T01:29:07.955678Z",
     "shell.execute_reply": "2026-01-20T01:29:07.955411Z"
    },
    "papermill": {
     "duration": 0.013744,
     "end_time": "2026-01-20T01:29:07.956422",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.942678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bit_expert_models = ['food','vehicle','instrument','flower','animal','object',\n",
    "                     'bird','mammal','arthropod','relation','abstraction']\n",
    "\n",
    "bit_expert_dictlist = []\n",
    "for expertise in bit_expert_models:\n",
    "    model = 'BiT-Expert-ResNet-V2-{}'.format(expertise.title())\n",
    "    dict_i = {'model': model, 'train_type': 'bit_expert', 'train_data': 'big_transfer'}\n",
    "    dict_i['architecture'] = 'ResNet50-V2'\n",
    "    dict_i['model_display_name'] = model\n",
    "    dict_i['model_class'] = 'Convolutional'\n",
    "    dict_i['description'] = ('{} trained first via ImageNet21K dataset, then as an expert on the {} subset.'\n",
    "                             .format(dict_i['architecture'], expertise.title()))\n",
    "    \n",
    "    bit_expert_dictlist.append(dict_i)\n",
    "    \n",
    "bit_expert_df = pd.DataFrame(bit_expert_dictlist)\n",
    "bit_expert_df['task_cluster'] = 'Expertise'\n",
    "bit_expert_df['model_source'] = 'bit_expert'\n",
    "bit_expert_df['model_source_url'] = 'https://tfhub.dev/google/collections/experts/bit/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709acc0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.964119Z",
     "iopub.status.busy": "2026-01-20T01:29:07.963999Z",
     "iopub.status.idle": "2026-01-20T01:29:07.973390Z",
     "shell.execute_reply": "2026-01-20T01:29:07.973159Z"
    },
    "papermill": {
     "duration": 0.013864,
     "end_time": "2026-01-20T01:29:07.974015",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.960151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Mammal</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Mammal</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Food</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Food</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Arthropod</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Arthropod</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  train_type    train_data architecture  \\\n",
       "7     BiT-Expert-ResNet-V2-Mammal  bit_expert  big_transfer  ResNet50-V2   \n",
       "0       BiT-Expert-ResNet-V2-Food  bit_expert  big_transfer  ResNet50-V2   \n",
       "8  BiT-Expert-ResNet-V2-Arthropod  bit_expert  big_transfer  ResNet50-V2   \n",
       "\n",
       "               model_display_name    model_class  \\\n",
       "7     BiT-Expert-ResNet-V2-Mammal  Convolutional   \n",
       "0       BiT-Expert-ResNet-V2-Food  Convolutional   \n",
       "8  BiT-Expert-ResNet-V2-Arthropod  Convolutional   \n",
       "\n",
       "                                         description task_cluster  \\\n",
       "7  ResNet50-V2 trained first via ImageNet21K data...    Expertise   \n",
       "0  ResNet50-V2 trained first via ImageNet21K data...    Expertise   \n",
       "8  ResNet50-V2 trained first via ImageNet21K data...    Expertise   \n",
       "\n",
       "  model_source                                   model_source_url  \n",
       "7   bit_expert  https://tfhub.dev/google/collections/experts/b...  \n",
       "0   bit_expert  https://tfhub.dev/google/collections/experts/b...  \n",
       "8   bit_expert  https://tfhub.dev/google/collections/experts/b...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit_expert_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25979e",
   "metadata": {
    "papermill": {
     "duration": 0.003387,
     "end_time": "2026-01-20T01:29:07.981223",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.977836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60f8cf3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:07.989379Z",
     "iopub.status.busy": "2026-01-20T01:29:07.989267Z",
     "iopub.status.idle": "2026-01-20T01:29:07.996807Z",
     "shell.execute_reply": "2026-01-20T01:29:07.996522Z"
    },
    "papermill": {
     "duration": 0.012253,
     "end_time": "2026-01-20T01:29:07.997535",
     "exception": false,
     "start_time": "2026-01-20T01:29:07.985282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_list = [torchvision_df, taskonomy_df, timm_df, clip_df, vissl_df, \n",
    "           dino_df, midas_df, yolo_df, detectron_df, timm_pluspretrain_df]\n",
    "\n",
    "custom_df_list = [slip_df, seer_df, ipcl_df, bit_expert_df]\n",
    "\n",
    "include_custom_models = True\n",
    "if include_custom_models:\n",
    "    df_list = df_list + custom_df_list\n",
    "    \n",
    "column_order = ['model','train_type','train_data','architecture','model_class','task_cluster',\n",
    "                'model_display_name', 'description', 'model_source', 'model_source_url','weights_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26d07330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:08.005777Z",
     "iopub.status.busy": "2026-01-20T01:29:08.005674Z",
     "iopub.status.idle": "2026-01-20T01:29:08.016803Z",
     "shell.execute_reply": "2026-01-20T01:29:08.016538Z"
    },
    "papermill": {
     "duration": 0.015697,
     "end_time": "2026-01-20T01:29:08.017472",
     "exception": false,
     "start_time": "2026-01-20T01:29:08.001775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "      <th>weights_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gernet_s</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>gernet_s</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>GerNet-S</td>\n",
       "      <td>GerNet-S trained on image classification with ...</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>gmixer_24_224</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>gmixer_24_224</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>GMixer-24</td>\n",
       "      <td>GMixer-24 randomly initialized, with no training.</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>ResNet50-PIRL, a self-supervised representatio...</td>\n",
       "      <td>vissl</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model      train_type train_data   architecture    model_class  \\\n",
       "34        gernet_s  classification   imagenet       gernet_s  Convolutional   \n",
       "162  gmixer_24_224          random       None  gmixer_24_224  Convolutional   \n",
       "5    ResNet50-PIRL  selfsupervised   imagenet       ResNet50  Convolutional   \n",
       "\n",
       "        task_cluster model_display_name  \\\n",
       "34          Semantic           GerNet-S   \n",
       "162         Semantic          GMixer-24   \n",
       "5    Self-Supervised      ResNet50-PIRL   \n",
       "\n",
       "                                           description model_source  \\\n",
       "34   GerNet-S trained on image classification with ...         timm   \n",
       "162  GMixer-24 randomly initialized, with no training.         timm   \n",
       "5    ResNet50-PIRL, a self-supervised representatio...        vissl   \n",
       "\n",
       "                                      model_source_url  \\\n",
       "34   https://github.com/rwightman/pytorch-image-mod...   \n",
       "162  https://github.com/rwightman/pytorch-image-mod...   \n",
       "5           https://github.com/facebookresearch/vissl/   \n",
       "\n",
       "                                           weights_url  \n",
       "34                                                 NaN  \n",
       "162                                                NaN  \n",
       "5    https://dl.fbaipublicfiles.com/vissl/model_zoo...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(df_list)[column_order].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "607536ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:08.025372Z",
     "iopub.status.busy": "2026-01-20T01:29:08.025254Z",
     "iopub.status.idle": "2026-01-20T01:29:08.045741Z",
     "shell.execute_reply": "2026-01-20T01:29:08.045467Z"
    },
    "papermill": {
     "duration": 0.025078,
     "end_time": "2026-01-20T01:29:08.046436",
     "exception": false,
     "start_time": "2026-01-20T01:29:08.021358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes to model typology\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_path = 'model_opts/model_typology.csv'\n",
    "if os.path.exists(output_path):\n",
    "    previous_df = pd.read_csv(output_path)\n",
    "    # check if previous df is the same as the current df\n",
    "    if previous_df.equals(pd.concat(df_list)[column_order]):\n",
    "        print('No changes to model typology')\n",
    "    else: # if there are changes, save the new df\n",
    "        print('Changes to model typology')\n",
    "        pd.concat(df_list)[column_order].to_csv(output_path, index=None)\n",
    "else: # if the file doesn't exist, save the df\n",
    "    pd.concat(df_list)[column_order].to_csv(output_path, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "neptune": {
   "notebookId": "9611440e-d6a3-485f-9618-c86b1b61c939",
   "projectVersion": 2
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.109449,
   "end_time": "2026-01-20T01:29:08.266119",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/colinconwell/GitHub/AffectExMachina/source/notebooks/model_typology.ipynb",
   "output_path": "/Users/colinconwell/GitHub/AffectExMachina/source/notebooks/model_typology.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T01:29:06.156670",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}